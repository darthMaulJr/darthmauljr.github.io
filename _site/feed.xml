<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sreejith's Notes</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 25 Apr 2023 00:43:40 -0700</pubDate>
    <lastBuildDate>Tue, 25 Apr 2023 00:43:40 -0700</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Learning the environment through Deep Learning</title>
        <description>&lt;p&gt;In the &lt;a href=&quot;https://srjit.github.io/elementary-blocks-of-robotics/&quot;&gt;previous post&lt;/a&gt; we discussed the basics of building a robot; mostly the electronics and mechanical components of it, with a little bit of the programming for basic motion. Here, in the following iteration we will be making the robot a little more intelligent with some machine learning!&lt;/p&gt;

&lt;p&gt;It was my friend’s suggestion to modify the robot’s construction and use a tank chassis since the wheels weren’t aligned enough resulting in sideways movement during forward motion. A tank track could also easily climb through the carpets and other uneven sufaces in my house. In addition, it can rotate or turn around itself any degree with minimal motion sideways. The picture shows the modified version of the robot. Although the mechanical parts have changed, this code that was used before is the same here to drive it.&lt;/p&gt;

&lt;h4 id=&quot;additional-hardware&quot;&gt;Additional Hardware&lt;/h4&gt;

&lt;h5 id=&quot;camera-module&quot;&gt;Camera Module&lt;/h5&gt;

&lt;p&gt;The camera that is being used here is &lt;a href=&quot;https://www.aliexpress.us/item/2255800029242375.html&quot;&gt;IMX219&lt;/a&gt;, an infrared camera with 160 Degree FOV. It is assisted by two IR Leds on either sides as seen in the picture. The picture quality from this camera isn’t great from my experience, however for our project this does decently well.&lt;/p&gt;

&lt;h5 id=&quot;wireless&quot;&gt;Wireless&lt;/h5&gt;

&lt;p&gt;For the wifi module, &lt;a href=&quot;https://www.amazon.com/gp/product/B07SM4SPLV&quot;&gt;this&lt;/a&gt; Dual Mode AC8265 Wireless NIC Module is being used. Since python modules need to be installed, and some prebuilt models need to be downloaded, having a steady wifi connection would be a plus!&lt;/p&gt;

&lt;h4 id=&quot;automating-movement&quot;&gt;Automating Movement&lt;/h4&gt;

&lt;h5 id=&quot;objective&quot;&gt;Objective&lt;/h5&gt;

&lt;p&gt;One of the issues with the previous version of the robot, as mentioned earlier was its uncontrolled sideways motion although it was set on a straight path. In my case, this resulted in the robot drifting slowly off onto my fur floor carpet and getting stuck, or running off to under my furniture.&lt;/p&gt;

&lt;p&gt;A little bit of computer vision based deep learning is used here to automate the motion of the robot.&lt;/p&gt;

&lt;h5 id=&quot;designing--implementing-the-solution&quot;&gt;Designing &amp;amp; Implementing the solution&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;Data Collection: I started off by defining a track for the robot, around the region of motion; in my case, this was my living room. To teach the robot about the track, plently of pictures were taken using the robot’s camera (what it could see during its motion). The pictures included parts of the normal track, what it’d see if it drifted sideways, or if it ran into some furniture.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some regions where the robot is expected to move forward (straight path besides the carpet):&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;../assets/images/room1.jpeg&quot; /&gt;
  &lt;img src=&quot;../assets/images/room2.jpeg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Some regions where the robot is expected to reverse its path (eg. About to get stuck near some furniture, or near my house plant):&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;../assets/images/room3.jpeg&quot; /&gt;
  &lt;img src=&quot;../assets/images/room4.jpeg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The idea here was to let the robot move a minimal amount of distance, capture a picture and let it decide which direction to move next. For training the first version of the decision making, I collected 500 images of my room.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The classifier: At least for the first version of decision making, I’ll be keeping things simple with a small multi-class classfier. The classifier has 4 classes (directions) - forward, reverse, turn-left, turn-right. To make things easy all the collected images were grouped into 4 different folders with the classname (direction) as foldername.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training: After a bunch of experiments coding and tuning classifiers, I turned to &lt;a href=&quot;https://en.wikipedia.org/wiki/Transfer_learning&quot;&gt;Transfer learning&lt;/a&gt;. Using PyTorch framework, I loaded AlexNet and changed the fully connected layer to return the number of classes (4) in this use case.
&lt;br /&gt;. The training code can be found &lt;a href=&quot;https://github.com/srjit/jetson-scripts/blob/main/01%20Environment/train-decision-making.ipynb&quot;&gt;here&lt;/a&gt;. The model reaches 99% accuracy in about 50 epochs. I wasn’t getting decent enough accuracy during early experiments. To check if I was losing accuracy for a specific class, I had the class level error rates printed out for each epoch. The best model was saved during training as &lt;code class=&quot;highlighter-rouge&quot;&gt;best_model_tank_v1.pth&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Coding the &lt;b&gt;Next-Step Decision Maker&lt;/b&gt;:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 12 Apr 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/Learning-an-environment/</link>
        <guid isPermaLink="true">http://localhost:4000/Learning-an-environment/</guid>
        
        <category>Machine Learning</category>
        
        <category>Robotics</category>
        
        <category>Programming</category>
        
        <category>Python</category>
        
        <category>Embedded</category>
        
        
      </item>
    
      <item>
        <title>Building Elementary Blocks of Robot Motion; Powered by Jetson Nano </title>
        <description>&lt;p&gt;Lately, I’ve been curious if embedded machine learing could make my life more fun, or even simplify it primarily along the lines of having a robot assistant or improve home automation. This blog post is about getting the basic hardware in shape for working towards that goal.&lt;/p&gt;

&lt;h4 id=&quot;choosing-the-processor&quot;&gt;Choosing the processor&lt;/h4&gt;

&lt;p&gt;When hobbists think about processors for embedded applications, the one that first comes to mind is the &lt;a href=&quot;https://www.raspberrypi.com/&quot;&gt;Raspberry Pi&lt;/a&gt;. However, supply chain issues in recent years have made it really hard to obtain one. For this reason, I started researching for alternate candidates and among them Jetson Nano was a clear winner. Jetson Nano is a small yet powerful computer developed by Nvidia Inc.  It can run AI frameworks and models, and is very power efficient. It is also supported by &lt;a href=&quot;https://developer.nvidia.com/embedded/jetpack&quot;&gt;Nvidia’s JetPack SDK&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition, it is priced reasonably for the hardware included!&lt;/p&gt;

&lt;p&gt;The robot we build here is similar to the &lt;a href=&quot;https://jetbot.org/master/&quot;&gt;Jetbot&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;building-the-chassis&quot;&gt;Building the Chassis&lt;/h4&gt;

&lt;p&gt;For the load bearing component, I used components from &lt;a href=&quot;https://www.amazon.com/Mixse-Tracking-Chassis-Compatible-Electric/dp/B08RMTJ8RP/&quot;&gt;this&lt;/a&gt; set. As seen in the pictures, the motion is controlled by the two wheels (motors) on either side and a rear-end freewheel. For the motors, instead of using the ones that came with this set, I’ve used &lt;a href=&quot;https://www.amazon.com/Augiimor-Reduction-Gearwheel-Gearbox-Electric/dp/B08B3L7T8D/&quot;&gt;these&lt;/a&gt; N20 motors that can withstand upto 6V.&lt;/p&gt;

&lt;h5 id=&quot;the-motor-driver-circuit&quot;&gt;The Motor Driver Circuit&lt;/h5&gt;

&lt;p&gt;The movement of these motors have to be controlled through logic (or a program); a few of Nano’s &lt;a href=&quot;https://jetsonhacks.com/2019/06/07/jetson-nano-gpio/&quot;&gt;GPIO&lt;/a&gt; pins needs to be programmed to pass turn signals. Since we have two motors for the bot, lets call them motor1 (Left) and motor2 (right). These motors are driven using the IC DRV8833 shown below. IN1 and IN2 provides turn direction signal for motor 1, and IN3 and IN4 for motor 2. The other inputs to the IC are Vcc and GND. We tap the unregulated 5V supply from GPIO pin 2 for Vcc voltage and a connection from pin 14 as the GND. OUT1 and OUT2 are the connections to motor1 and OUT3 and OUT4 are the connections to motor2. This circuit, wired on the breadboard is shown below. The voltages from even numbered GPIO pins (second row of the GPIO pins) has been wired to the breadboard using a &lt;a href=&quot;https://www.amazon.com/dp/B08T9HCL37&quot;&gt;ribbon cable&lt;/a&gt;&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;../assets/images/motor-driver.jpg&quot; /&gt;
  &lt;img src=&quot;../assets/images/breadboard.jpeg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;De-complicating this circuit after removing the breadboard and the ribbon cable, we have something that looks like the following left image. The image on the right is the same chassis, cleaned up a bit further to smooth motion.&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;../assets/images/motors.jpeg&quot; /&gt;
  &lt;img src=&quot;../assets/images/clean_chassis.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;powering-the-processor&quot;&gt;Powering the processor&lt;/h4&gt;

&lt;p&gt;Since the robot has to move around its environment, a portable power source is inevitable. I am using the &lt;a href=&quot;https://www.amazon.com/dp/B08BRMZ4G6&quot;&gt;Waveshare UPS Power Module&lt;/a&gt; for Jetson Nano. This UPS needs four &lt;a href=&quot;https://www.amazon.com/dp/B0BC947XDJ&quot;&gt;18650&lt;/a&gt; batteries. It comes with a small OLED display that provides information on batteries voltage, IP address, RAM usage, etc. collected via I2C pins (SDA and SDL). For activating this display, a &lt;a href=&quot;https://www.waveshare.com/wiki/UPS_Power_Module&quot;&gt;service&lt;/a&gt; has to be enabled on the jetson OS. The UPS mounted on the chassis is shown below.&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;../assets/images/ups.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;After placing the UPS, the board has been affixed on to the top layer of the chassis.&lt;/p&gt;

&lt;h4 id=&quot;driving-the-motors-using-gpio-controls&quot;&gt;Driving the motors using GPIO Controls.&lt;/h4&gt;

&lt;p&gt;Now that the body of the robot has been built, it is time to program the logic for its movement! The Nano has a 40 pin &lt;a href=&quot;https://jetsonhacks.com/2019/06/07/jetson-nano-gpio/&quot;&gt;GPIO&lt;/a&gt; layout quite similar to the Raspberry Pi. Enabling voltage on selected pins and wiring them as input signals to the motors is how the robot motion would be controlled.&lt;/p&gt;

&lt;p&gt;The GPIO pins numbered 12, 16, 18 and 22 are the ones being used here for motion control. As mentioned earlier, IN1 and IN2 are the inputs of the motor driver chip for motor1. GPIO pins 12 and 16 has been wired to IN1 and IN2 to control the direction of motor1. Similarly, pins 18 and 22 are wired to IN3 and IN4 for controlling motor2. A logical high on IN1 (pin 12) drives motor1 forward whereas a logical high on IN2 (pin 16) drives it backward. Similarily, a logical high on IN4 (pin 22) drives the motor forward whereas that on IN3 (pin 18) drives it backward.&lt;/p&gt;

&lt;h4 id=&quot;the-code&quot;&gt;The Code!&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;For driving the robot forward, motor1 and motor2 needs to be turned in the forward direction, i.e we have to set logical highs on GPIO pins 12 and 22 (IN1 and IN4). For driving it back, motor1 and motor2 needs to be turned in the reverse directions, i.e pins 16 and 18 (IN2 and IN3) needs to set high.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/NVIDIA/jetson-gpio&quot;&gt;Jetson GPIO&lt;/a&gt; python library is what is used here to change the signals on board pins. We use the BCM pin-numbering scheme from Raspberry Pi while programming the GPIO; there is a mapping of each board pin to a different BCM number.&lt;/p&gt;

&lt;p&gt;The following python code moves the robot 1 second forward and reverses 5 seconds!&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;---
import subprocess

import RPi.GPIO as GPIO
import time
import nanocamera as nano
from PIL import Image
import sys

# Pin Definitions
output_pin1 = 18  # BCM pin 18, BOARD pin 12
output_pin2 = 23  # BCM pin 23, BOARD pin 16

output_pin3 = 24  # BCM pin 24, BOARD pin 18
output_pin4 = 25  # BCM pin 25, BOARD pin 22


def turn_right(turn_time):

    GPIO.output(output_pin1, GPIO.HIGH)
    time.sleep(turn_time)     
    GPIO.output(output_pin1, GPIO.LOW)


def turn_left(turn_time):

    GPIO.output(output_pin4, GPIO.HIGH)
    time.sleep(turn_time)     
    GPIO.output(output_pin4, GPIO.LOW)



def forward(running_time):

    GPIO.output(output_pin1, GPIO.HIGH)
    GPIO.output(output_pin4, GPIO.HIGH)
    time.sleep(running_time)
    GPIO.output(output_pin1, GPIO.LOW)
    GPIO.output(output_pin4, GPIO.LOW)
    


def reverse(running_time):    

    GPIO.output(output_pin2, GPIO.HIGH)
    GPIO.output(output_pin3, GPIO.HIGH)
    time.sleep(running_time)
    GPIO.output(output_pin2, GPIO.LOW)
    GPIO.output(output_pin3, GPIO.LOW)

def initialize():

    GPIO.setmode(GPIO.BCM)
    GPIO.setup(output_pin1, GPIO.OUT, initial=GPIO.LOW)
    GPIO.setup(output_pin2, GPIO.OUT, initial=GPIO.LOW)
    GPIO.setup(output_pin3, GPIO.OUT, initial=GPIO.LOW)
    GPIO.setup(output_pin4, GPIO.OUT, initial=GPIO.LOW)
    


def main(start):
    # Pin Setup:

    initialize()

    # Move forward for 1 second and stop
    forward(1)

    # Move back for 5 seconds and stop
    
---
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 30 Mar 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/elementary-blocks-of-robotics/</link>
        <guid isPermaLink="true">http://localhost:4000/elementary-blocks-of-robotics/</guid>
        
        <category>Robotics</category>
        
        <category>Programming</category>
        
        <category>Python</category>
        
        <category>Embedded</category>
        
        
      </item>
    
      <item>
        <title>Let's test spoilers</title>
        <description>&lt;p&gt;Director Roland Suso Richter’s enigmatic psychological thriller (direct to video/DVD) was based upon screenwriter Michael Cooney’s own play “Point of Death” - a title that gave away the film’s entire plot twist premise.&lt;/p&gt;

&lt;p&gt;As in many similar films, such as Jacob’s Ladder (1990), Soul Survivors (2001), and The Butterfly Effect (2004), events and people were thoroughly distorted and confused because the protagonist was at the point of death. The tagline was misleading:&lt;/p&gt;

&lt;p&gt;“When You Don’t Have a Memory, How Can You Remember Who to Trust?”&lt;/p&gt;

&lt;p&gt;The mind-warping film opened with a hospital patient Simon Cable (Ryan Phillippe) awakening in a &lt;span class=&quot;spoiler&quot;&gt; hospital with little knowledge (amnesia perhaps?) of what had happened, and why he was there, etc. He was told by attending Dr. Jeremy Newman (Stephen Rea) that it was July 29, 2002 (Simon thought it was the year 2000 - he was confused - he heard a doctor say 20:00 hours!) and that he had died for two minutes from cardiac arrest following the near-fatal accident – but he had been revived (“You’re as good as new”).&lt;/span&gt; Dr. Newman: “Simon, this is the 29th of July. The year is 2002. And your wife, whose name is Anna, is waiting outside.”&lt;/p&gt;

&lt;p&gt;(The doctor left off four crucial additional words, revealed in the film’s ending.) (Spoiler: Simon had died and was not resuscitated!).&lt;/p&gt;

&lt;p&gt;A major clue to everything that truly happened was the scene that played next under the credits - hospital staff failed to bring a patient back to life with a defibrillator after a car accident. Chest compressions failed and there was no pulse. A second major clue was provided by hospital orderly Travis (Stephen Graham): &lt;span class=&quot;spoiler&quot;&gt;Everybody dies. No mystery there. But why and how everyone dies. Now, there’s a mystery worth solving. Probably the biggest mystery there is.&lt;/span&gt;&lt;/p&gt;

&lt;h4 id=&quot;so-how-do-we-do-spoilers&quot;&gt;So how do we do spoilers?&lt;/h4&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;spoiler&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;My hidden paragraph here.&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 11 Jan 2018 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/quick-start-guide/</link>
        <guid isPermaLink="true">http://localhost:4000/quick-start-guide/</guid>
        
        
        <category>Jekyll</category>
        
        <category>tutorial</category>
        
      </item>
    
  </channel>
</rss>

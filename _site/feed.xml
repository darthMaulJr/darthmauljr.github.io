<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sreejith&apos;s Notes</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 16 May 2023 11:28:54 -0700</pubDate>
    <lastBuildDate>Tue, 16 May 2023 11:28:54 -0700</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Adding (Sarcastic) Voice Interaction Capabilities to the Robot</title>
        <description>&lt;p&gt;Coming Soon…&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Apr 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/adding-sarcastic-voice-interactions/</link>
        <guid isPermaLink="true">http://localhost:4000/adding-sarcastic-voice-interactions/</guid>
        
        <category>Artificial Intelligence</category>
        
        <category>LLMs</category>
        
        <category>Robotics</category>
        
        <category>Programming</category>
        
        <category>Python</category>
        
        
      </item>
    
      <item>
        <title>Learning the environment through Deep Learning</title>
        <description>&lt;p&gt;In the &lt;a href=&quot;https://srjit.github.io/elementary-blocks-of-robotics/&quot;&gt;previous post&lt;/a&gt; we discussed the basics of building a robot; mostly the electronics and mechanical components of it, with a little bit of the programming for basic motion. Here, in the following iteration we will be making the robot a little more intelligent with some machine learning!&lt;/p&gt;

&lt;p&gt;It was my friend’s suggestion to modify the robot’s construction and use a tank chassis since the wheels weren’t aligned enough resulting in sideways movement during forward motion. A tank track could also easily climb through the carpets and other uneven sufaces in my house. In addition, it can rotate or turn around itself any degree with minimal motion sideways. The picture shows the modified version of the robot. Although the mechanical parts have changed, this code that was used before is the same here to drive it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bot_tank.jpeg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;additional-hardware&quot;&gt;&lt;strong&gt;Additional Hardware&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;camera-module&quot;&gt;&lt;strong&gt;Camera Module&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The camera that is being used here is &lt;a href=&quot;https://www.aliexpress.us/item/2255800029242375.html&quot;&gt;IMX219&lt;/a&gt;, an infrared camera with 160 Degree FOV. It is assisted by two IR Leds on either sides as seen in the picture. The picture quality from this camera isn’t great from my experience, however for our project this does decently well.&lt;/p&gt;

&lt;h3 id=&quot;wireless&quot;&gt;&lt;strong&gt;Wireless&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;For the wifi module, &lt;a href=&quot;https://www.amazon.com/gp/product/B07SM4SPLV&quot;&gt;this&lt;/a&gt; Dual Mode AC8265 Wireless NIC Module is being used. Since python modules need to be installed, and some prebuilt models need to be downloaded, having a steady wifi connection would be a plus!&lt;/p&gt;

&lt;h2 id=&quot;automating-movement&quot;&gt;&lt;strong&gt;Automating Movement&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;objective&quot;&gt;&lt;strong&gt;Objective&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;One of the issues with the previous version of the robot, as mentioned earlier was its uncontrolled sideways motion although it was set on a straight path. In my case, this resulted in the robot drifting slowly off onto my fur floor carpet and getting stuck, or running off to under my furniture.&lt;/p&gt;

&lt;p&gt;A little bit of computer vision based deep learning is used here to automate the motion of the robot.&lt;/p&gt;

&lt;h3 id=&quot;designing--implementing-the-solution&quot;&gt;&lt;strong&gt;Designing &amp;amp; Implementing the solution&lt;/strong&gt;&lt;/h3&gt;

&lt;ol class=&quot;ol-post&quot;&gt;

&lt;li&gt; Data Collection: I started off by defining a track for the robot, around the region of motion; in my case, this was my living room. To teach the robot about the track, plently of pictures were taken using the robot&apos;s camera (what it could see during its motion). The pictures included parts of the normal track, what it&apos;d see if it drifted sideways, or if it ran into some furniture.

Some regions where the robot is expected to move forward (straight path besides the carpet):

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/room1.jpeg&quot; /&gt;
  &lt;img src=&quot;/assets/images/room2.jpeg&quot; /&gt;
&lt;/div&gt;

&lt;br /&gt;

Some regions where the robot is expected to reverse its path (eg. About to get stuck near some furniture, or near my house plant):

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/room3.jpeg&quot; /&gt;
  &lt;img src=&quot;/assets/images/room4.jpeg&quot; /&gt;
&lt;/div&gt;

The idea here was to let the robot move a minimal amount of distance, capture a picture and let it decide which direction to move next. For training the first version of the decision making, I collected 500 images of my room.

&lt;/li&gt;

&lt;li&gt; The classifier: At least for the first version of decision making, I&apos;ll be keeping things simple with a small multi-class classfier. The classifier has 4 classes (directions) - forward, reverse, turn-left, turn-right. To make things easy all the collected images were grouped into 4 different folders with the classname (direction) as foldername. &lt;/li&gt;

&lt;li&gt; Training: After a bunch of experiments coding and tuning classifiers, I turned to &lt;a href=&quot;https://en.wikipedia.org/wiki/Transfer_learning&quot;&gt;Transfer learning&lt;/a&gt;. Using PyTorch framework, I loaded AlexNet and changed the fully connected layer to return the number of classes (4) in this use case.
&lt;br /&gt;. The training code can be found &lt;a href=&quot;https://github.com/srjit/jetson-scripts/blob/main/01%20Environment/train-decision-making.ipynb&quot;&gt;here&lt;/a&gt;. The model reaches 99% accuracy in about 50 epochs. I wasn&apos;t getting decent enough accuracy during early experiments. To check if I was losing accuracy for a specific class, I had the class level error rates printed out for each epoch. The best model was saved during training as `best_model_tank_v1.pth`. &lt;/li&gt;

&lt;li&gt; Coding the &lt;b&gt;Next-Step Decision Maker&lt;/b&gt;: The code for testing the motion of the robot controlled by this neural net classifier can be found &lt;a href=&quot;https://github.com/srjit/jetson-scripts/blob/main/01%20Environment/living_room_track.py&quot;&gt;here&lt;/a&gt;. As mentioned earlier, we make the motion in discrete steps. At each &lt;a href=&quot;https://github.com/srjit/jetson-scripts/blob/316c835b9fadadbc13f7c49f85b97bf4c6805c29/01%20Environment/living_room_track.py#L110&quot;&gt;step&lt;/a&gt; of the motion, the robot &lt;a href=&quot;https://github.com/srjit/jetson-scripts/blob/316c835b9fadadbc13f7c49f85b97bf4c6805c29/01%20Environment/living_room_track.py#L115&quot;&gt;captures&lt;/a&gt; an image, and decides an action for the next time step. The captured image is received as a numpy array parameter to &lt;a href=&quot;https://github.com/srjit/jetson-scripts/blob/316c835b9fadadbc13f7c49f85b97bf4c6805c29/01%20Environment/decision_engine.py#L43&quot;&gt;this&lt;/a&gt; function, where it undergoes all the pre-processing which was done during the training of the classifier. The engine module also takes care of &lt;a href=&quot;https://github.com/srjit/jetson-scripts/blob/main/01%20Environment/decision_engine.py#L22&quot;&gt;loading&lt;/a&gt; the previously saved weights from our best model during training, and setting it to eval mode for deployement. The classfied output corresponds to an index that maps to one of the four levels. Returning back to our motion iteration, if the decision made is `forward`, the robot moves 0.4 seconds straight forward etc. This part of the code is particularily not intelligent, and has been customized according to the time taken by the robot for 90 degree sideways turns. Moving the robot in different directions has been detailed in the previous &lt;a href=&quot;https://srjit.github.io/elementary-blocks-of-robotics/&quot;&gt;post&lt;/a&gt;. &lt;/li&gt;

&lt;li&gt; Testing/ Deployment: A small portion of the robot motion guided by the classifier can be seen here: &lt;a href=&quot;https://youtube.com/shorts/DHARDDDx3bk?feature=share&quot;&gt;Clip 1: Robot Motion&lt;/a&gt;. Also, here is a short clip of the robot detecting objects in its path and choosing to move back to form a different path in the next timestep: &lt;a href=&quot;https://youtube.com/shorts/d4d3PlKnxdY?feature=share&quot;&gt; Clip 2: Avoiding Collisions&lt;/a&gt;. &lt;/li&gt;

&lt;/ol&gt;
</description>
        <pubDate>Tue, 25 Apr 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/Learning-an-environment/</link>
        <guid isPermaLink="true">http://localhost:4000/Learning-an-environment/</guid>
        
        <category>Machine Learning</category>
        
        <category>Robotics</category>
        
        <category>Programming</category>
        
        <category>Python</category>
        
        <category>Embedded</category>
        
        
      </item>
    
      <item>
        <title>Building Elementary Blocks of Robot Motion; Powered by Jetson Nano </title>
        <description>&lt;p&gt;Lately, I’ve been curious if embedded machine learing could make my life more fun, or even simplify it primarily along the lines of having a robot assistant or improve home automation. This blog post is about getting the basic hardware in shape for working towards that goal.&lt;/p&gt;

&lt;h3 id=&quot;choosing-the-processor&quot;&gt;&lt;strong&gt;Choosing the processor&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;When hobbists think about processors for embedded applications, the one that first comes to mind is the &lt;a href=&quot;https://www.raspberrypi.com/&quot;&gt;Raspberry Pi&lt;/a&gt;. However, supply chain issues in recent years have made it really hard to obtain one. For this reason, I started researching for alternate candidates and among them Jetson Nano was a clear winner. Jetson Nano is a small yet powerful computer developed by Nvidia Inc.  It can run AI frameworks and models, and is very power efficient. It is also supported by &lt;a href=&quot;https://developer.nvidia.com/embedded/jetpack&quot;&gt;Nvidia’s JetPack SDK&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition, it is priced reasonably for the hardware included!&lt;/p&gt;

&lt;p&gt;The robot we build here is similar to the &lt;a href=&quot;https://jetbot.org/master/&quot;&gt;Jetbot&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;building-the-chassis&quot;&gt;&lt;strong&gt;Building the Chassis&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;For the load bearing component, I used components from &lt;a href=&quot;https://www.amazon.com/Mixse-Tracking-Chassis-Compatible-Electric/dp/B08RMTJ8RP/&quot;&gt;this&lt;/a&gt; set. As seen in the pictures, the motion is controlled by the two wheels (motors) on either side and a rear-end freewheel. For the motors, instead of using the ones that came with this set, I’ve used &lt;a href=&quot;https://www.amazon.com/Augiimor-Reduction-Gearwheel-Gearbox-Electric/dp/B08B3L7T8D/&quot;&gt;these&lt;/a&gt; N20 motors that can withstand upto 6V.&lt;/p&gt;

&lt;h3 id=&quot;the-motor-driver-circuit&quot;&gt;&lt;strong&gt;The Motor Driver Circuit&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The movement of these motors have to be controlled through logic (or a program); a few of Nano’s &lt;a href=&quot;https://jetsonhacks.com/2019/06/07/jetson-nano-gpio/&quot;&gt;GPIO&lt;/a&gt; pins needs to be programmed to pass turn signals. Since we have two motors for the bot, lets call them motor1 (Left) and motor2 (right). These motors are driven using the IC DRV8833 shown below. IN1 and IN2 provides turn direction signal for motor 1, and IN3 and IN4 for motor 2. The other inputs to the IC are Vcc and GND. We tap the unregulated 5V supply from GPIO pin 2 for Vcc voltage and a connection from pin 14 as the GND. OUT1 and OUT2 are the connections to motor1 and OUT3 and OUT4 are the connections to motor2. This circuit, wired on the breadboard is shown below. The voltages from even numbered GPIO pins (second row of the GPIO pins) has been wired to the breadboard using a &lt;a href=&quot;https://www.amazon.com/dp/B08T9HCL37&quot;&gt;ribbon cable&lt;/a&gt;&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/motor-driver.jpg&quot; /&gt;
  &lt;img src=&quot;/assets/images/breadboard.jpeg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;De-complicating this circuit after removing the breadboard and the ribbon cable, we have something that looks like the following left image. The image on the right is the same chassis, cleaned up a bit further to smooth motion.&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/motors.jpeg&quot; /&gt;
  &lt;img src=&quot;/assets/images/clean_chassis.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;powering-the-processor&quot;&gt;&lt;strong&gt;Powering the processor&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Since the robot has to move around its environment, a portable power source is inevitable. I am using the &lt;a href=&quot;https://www.amazon.com/dp/B08BRMZ4G6&quot;&gt;Waveshare UPS Power Module&lt;/a&gt; for Jetson Nano. This UPS needs four &lt;a href=&quot;https://www.amazon.com/dp/B0BC947XDJ&quot;&gt;18650&lt;/a&gt; batteries. It comes with a small OLED display that provides information on batteries voltage, IP address, RAM usage, etc. collected via I2C pins (SDA and SDL). For activating this display, a &lt;a href=&quot;https://www.waveshare.com/wiki/UPS_Power_Module&quot;&gt;service&lt;/a&gt; has to be enabled on the jetson OS. The UPS mounted on the chassis is shown below.&lt;/p&gt;

&lt;div align=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/ups.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;After placing the UPS, the board has been affixed on to the top layer of the chassis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bot.jpeg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;driving-the-motors-using-gpio-controls&quot;&gt;&lt;strong&gt;Driving the motors using GPIO Controls&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Now that the body of the robot has been built, it is time to program the logic for its movement! The Nano has a 40 pin &lt;a href=&quot;https://jetsonhacks.com/2019/06/07/jetson-nano-gpio/&quot;&gt;GPIO&lt;/a&gt; layout quite similar to the Raspberry Pi. Enabling voltage on selected pins and wiring them as input signals to the motors is how the robot motion would be controlled.&lt;/p&gt;

&lt;p&gt;The GPIO pins numbered 12, 16, 18 and 22 are the ones being used here for motion control. As mentioned earlier, IN1 and IN2 are the inputs of the motor driver chip for motor1. GPIO pins 12 and 16 has been wired to IN1 and IN2 to control the direction of motor1. Similarly, pins 18 and 22 are wired to IN3 and IN4 for controlling motor2. A logical high on IN1 (pin 12) drives motor1 forward whereas a logical high on IN2 (pin 16) drives it backward. Similarily, a logical high on IN4 (pin 22) drives the motor forward whereas that on IN3 (pin 18) drives it backward.&lt;/p&gt;

&lt;h3 id=&quot;the-code&quot;&gt;&lt;strong&gt;The Code!&lt;/strong&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;For driving the robot forward, motor1 and motor2 needs to be turned in the forward direction, i.e we have to set logical highs on GPIO pins 12 and 22 (IN1 and IN4). For driving it back, motor1 and motor2 needs to be turned in the reverse directions, i.e pins 16 and 18 (IN2 and IN3) needs to set high.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/NVIDIA/jetson-gpio&quot;&gt;Jetson GPIO&lt;/a&gt; python library is what is used here to change the signals on board pins. We use the BCM pin-numbering scheme from Raspberry Pi while programming the GPIO; there is a mapping of each board pin to a different BCM number.&lt;/p&gt;

&lt;p&gt;The following python code moves the robot 1 second forward and reverses 5 seconds!&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
import subprocess

import RPi.GPIO as GPIO
import time
import nanocamera as nano
from PIL import Image
import sys

# Pin Definitions
output_pin1 = 18  # BCM pin 18, BOARD pin 12
output_pin2 = 23  # BCM pin 23, BOARD pin 16

output_pin3 = 24  # BCM pin 24, BOARD pin 18
output_pin4 = 25  # BCM pin 25, BOARD pin 22


def turn_right(turn_time):

    GPIO.output(output_pin1, GPIO.HIGH)
    time.sleep(turn_time)     
    GPIO.output(output_pin1, GPIO.LOW)


def turn_left(turn_time):

    GPIO.output(output_pin4, GPIO.HIGH)
    time.sleep(turn_time)     
    GPIO.output(output_pin4, GPIO.LOW)



def forward(running_time):

    GPIO.output(output_pin1, GPIO.HIGH)
    GPIO.output(output_pin4, GPIO.HIGH)
    time.sleep(running_time)
    GPIO.output(output_pin1, GPIO.LOW)
    GPIO.output(output_pin4, GPIO.LOW)
    


def reverse(running_time):    

    GPIO.output(output_pin2, GPIO.HIGH)
    GPIO.output(output_pin3, GPIO.HIGH)
    time.sleep(running_time)
    GPIO.output(output_pin2, GPIO.LOW)
    GPIO.output(output_pin3, GPIO.LOW)

def initialize():

    GPIO.setmode(GPIO.BCM)
    GPIO.setup(output_pin1, GPIO.OUT, initial=GPIO.LOW)
    GPIO.setup(output_pin2, GPIO.OUT, initial=GPIO.LOW)
    GPIO.setup(output_pin3, GPIO.OUT, initial=GPIO.LOW)
    GPIO.setup(output_pin4, GPIO.OUT, initial=GPIO.LOW)
    


def main(start):
    # Pin Setup:

    initialize()

    # Move forward for 1 second and stop
    forward(1)

    # Move back for 5 seconds and stop
    
---
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 30 Mar 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/elementary-blocks-of-robotics/</link>
        <guid isPermaLink="true">http://localhost:4000/elementary-blocks-of-robotics/</guid>
        
        <category>Robotics</category>
        
        <category>Programming</category>
        
        <category>Python</category>
        
        <category>Embedded</category>
        
        
      </item>
    
  </channel>
</rss>
